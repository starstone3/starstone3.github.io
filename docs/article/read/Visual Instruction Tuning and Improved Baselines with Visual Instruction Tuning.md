---
comments : true
hide:
    - feedback
---
# [Visual Instruction Tuning与Improved Baselines with Visual Instruction Tuning](https://llava-vl.github.io/)

这里也出现了一段话：**Finally, note that visual instruction tuning is different from visual prompt tuning : the former aims to improve the model’s instruction-following abilities, while the latter aims to improve the parameter-efficiency in model adaptation**

但没有太理解，GPT解释如下：

+ 视觉指令调优（Visual Instruction Tuning）
  + 定义：视觉指令调优是一种方法，旨在通过调整模型，使其能够更好地理解和执行给定的指令。指令通常是自然语言形式的，比如“识别图像中的猫”或“解释图片中的场景”
  + 目标：提高模型在接收指令后的执行能力，也就是让模型能够更准确地按照指令去处理和分析视觉内容。
  + Example: 假设你正在开发一个智能相册应用，用户可以通过语音指令搜索他们的照片，比如“显示我在海边的所有照片”或者“找到我生日派对的照片”。模型需要从用户提供的语音或文字指令中理解要查找的内容，然后在大量照片中找到符合条件的图片。你会用大量类似的指令-照片对来训练模型，例如“在照片中找到穿红衣服的人”以及相关的图片。通过不断训练，模型变得更擅长理解这些自然语言指令，并在照片中找到相关的内容。

+ 视觉提示调优（Visual Prompt Tuning）
  + 定义：视觉提示调优则侧重于通过提示（prompts）来调整模型的行为。这些提示可能是一些预先设定的输入，帮助模型适应新的任务或数据。这种方法通常用于在不大幅修改模型参数的情况下，让模型适应新任务.
  + 目标：提高模型在新任务中的参数效率，即在尽量少修改模型参数的情况下，使模型能够适应新的任务或数据。
  + 你有一个已经训练好的模型，能够识别基本的物体，如猫、狗、车等。现在，你想让这个模型能够识别特定猫的品种，但不想从头开始重新训练整个模型。你有一个已经训练好的模型，能够识别基本的物体，如猫、狗、车等。现在，你想让这个模型能够识别特定猫的品种，但不想从头开始重新训练整个模型。你会设计一些特定的“提示”，比如给模型输入一些典型的猫品种图片，或者使用一些带有品种标签的特征向量。这些提示不会改变模型的核心结构或大量参数，但会引导模型在做出预测时更加关注与猫品种相关的特征。你会设计一些特定的“提示”，比如给模型输入一些典型的猫品种图片，或者使用一些带有品种标签的特征向量。这些提示不会改变模型的核心结构或大量参数，但会引导模型在做出预测时更加关注与猫品种相关的特征。

文章提到了使用GPT-4生成测试数据集，在之前的其他地方也看到过，似乎是个挺常见的方法？

!!! info "pipeline猜测"
    详见LLaVA/llava/train/train.py这个九百多行，承担了主要训练任务的文件。
    LLaVA 的 pipeline 总体可以分为以下几个步骤：

    1. 多模态信息的有效融合
        + 图像和文本的结合：LLaVA 成功的核心原因之一在于它能够有效地将视觉信息（图像）和文本信息融合在一起。这是通过引入如 DEFAULT_IMAGE_TOKEN 和专门处理图像相关 token 的机制来实现的。它能够处理文本中嵌入的图像信息，并通过视觉和语言的协同学习，增强模型的理解和推理能力。
        + 多模态投影器 (mm_projector)：LLaVA 利用了专门的多模态投影层，将视觉特征投影到语言空间，使得图像信息能够自然地融入语言模型的上下文。这种结合使模型能够基于视觉信息生成语言内容，或者对多模态输入做出合理的推理。
    2. 智能的数据采样和处理机制
        + 长度分组采样 (LengthGroupedSampler)：在多模态数据处理中，LLaVA 引入了按长度分组采样的机制，这保证了在训练过程中，不同长度的样本能够被均匀处理，并提高了批次构建的效率。对于视觉和语言混合的数据，这种采样策略避免了极端样本（如极长或极短的样本）对训练效果的干扰，优化了训练效率。
        + 延迟数据加载：LLaVA 使用 LazySupervisedDataset 实现数据的延迟加载，减少了内存消耗，并在处理大规模数据集时保持高效的处理速度。这使得在多模态数据集训练时能够支持更大规模的数据量。
    3. 灵活的优化器与模型训练设计
        + 参数优化策略：LLaVA 针对不同的模型部分（如多模态投影器、语言模型等）设定了不同的优化策略。通过设定不同的学习率和权重衰减参数，模型可以针对语言和视觉部分进行微调，达到更好的训练效果。
        + 微调与保存机制：模型在保存时仅保留与多模态相关的关键参数，如 mm_projector，这在不牺牲性能的前提下大幅减少了模型的存储负担，同时也使得多模态相关部分的微调更为有效。
    4. 优秀的框架设计与深度学习技术
        + 基于 Transformers 框架：LLaVA 充分利用了 Transformers 框架的优势，如分布式训练、智能优化器选择等。Transformer 的自注意力机制使得模型能够有效捕捉输入中跨模态的信息交互，并在上下文中合理整合视觉和语言数据。
        + 深度学习的前沿技术：模型整合了多项深度学习技术，如 DeepSpeed 优化器的支持和 8-bit Adam 优化技术等。这些优化器能进一步提升大规模训练的性能，并降低训练的显存占用。
    5. 针对多模态任务的精细设计
        + 特定任务处理：LLaVA 的设计理念包括了针对不同任务（如视觉问答、图像描述等）的特定模块优化。通过对多模态任务的精细处理，模型能够根据上下文更好地推理，从而在多模态任务中表现出色。
        + 适应性强的架构：LLaVA 的架构支持不同类型的输入数据，并通过灵活的训练流程和优化策略，使得它能够在多种场景和任务中进行扩展和应用。